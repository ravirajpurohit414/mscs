{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34d1475c",
   "metadata": {},
   "source": [
    "# ------------------------- CSE-6363-001 ML Project 2 -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4bca4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b22c55",
   "metadata": {},
   "source": [
    "## ------------------------- Loading Data and Preprocessing -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7ed8e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename='./data/data.txt'):\n",
    "    \"\"\"\n",
    "    Load the dataset provided with the homework.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename - string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data - numpy array of floats\n",
    "    labels - numpy array of integers\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            ## remove noise from the row of data and separate features and labels\n",
    "            line = line.strip().replace('\\n','').split(',')\n",
    "            features.append([float(i) for i in line[:4]])\n",
    "            labels.append(line[-1])\n",
    "            \n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23659f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (120,)\n"
     ]
    }
   ],
   "source": [
    "features, labels = read_data()\n",
    "print(f\"shape of features - {features.shape}, \\nshape of labels - {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99ff1dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "label categories are - ['Ceramic', 'Metal', 'Plastic']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nlabel categories are - {[i for i in np.unique(labels)]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2762ee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(labels):\n",
    "    \"\"\"\n",
    "    perform one hot encoding to the labels\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    labels - numpy array of strings\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    labels_encoded - numpy array of one-hot-encoded labels\n",
    "    \n",
    "    \"\"\"\n",
    "    num_samples = len(labels)\n",
    "    labels_encoded = np.zeros((num_samples, len(np.unique(labels))))\n",
    "    for i in range(num_samples):\n",
    "        if labels[i] == 'Plastic':\n",
    "            labels_encoded[i, 0] = 1\n",
    "        elif labels[i] == 'Metal':\n",
    "            labels_encoded[i, 1] = 1\n",
    "        elif labels[i] == 'Ceramic':\n",
    "            labels_encoded[i, 2] = 1\n",
    "            \n",
    "    return labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da549e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_enc = one_hot_encoding(labels)\n",
    "print(labels_enc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2caf421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(features, labels):\n",
    "    \"\"\"\n",
    "    split the dataset into train-test randomly in 75:25 for training and testing respectively\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    features - numpy array of floats\n",
    "    labels - numpy array of strings\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    train_features - numpy array of floats\n",
    "    test_features - numpy array of floats\n",
    "    train_labels - numpy array of strings\n",
    "    test_labels - numpy array of strings\n",
    "    \n",
    "    \"\"\"\n",
    "    # Shuffle the indices\n",
    "    np.random.seed(21)\n",
    "    shuffled_indices = np.random.permutation(len(features))\n",
    "    \n",
    "    # Split the shuffled indices into train and test sets\n",
    "    train_indices = shuffled_indices[:int(len(features) * 0.75)]\n",
    "    test_indices = shuffled_indices[int(len(features) * 0.75):]\n",
    "    \n",
    "    # Use the train and test indices to split the features and labels\n",
    "    train_features = features[train_indices]\n",
    "    train_labels = labels[train_indices]\n",
    "    test_features = features[test_indices]\n",
    "    test_labels = labels[test_indices]\n",
    "    \n",
    "    return train_features, train_labels, test_features, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3560b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels, test_features, test_labels = train_test_split(features, labels_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dfd08ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of training dataset: features - (90, 4), labels - (90, 3)\n",
      "shape of testing dataset: features - (30, 4), labels - (30, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f'shape of training dataset: features - {train_features.shape}, labels - {train_labels.shape}')\n",
    "print(f'shape of testing dataset: features - {test_features.shape}, labels - {test_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c729db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f93d49f",
   "metadata": {},
   "source": [
    "## ------------------------- Question 1 (A,B) -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97292aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- Question 1 (A,B) -------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------- Question 1 (A,B) -------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba1660d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Applies the softmax function element-wise to each row of the input array.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z - a numpy array of shape (n_samples, n_classes) containing the output \n",
    "    of a linear transformation of the input data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res - a numpy array of shape (n_samples, n_classes) containing the \n",
    "    probabilities of each sample belonging to each class, \n",
    "    computed using the softmax function.\n",
    "    \"\"\"\n",
    "    e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    res = e_z / e_z.sum(axis=1, keepdims=True)\n",
    "\n",
    "    return res\n",
    "\n",
    "def initialize_params(dim):\n",
    "    \"\"\"\n",
    "    Initializes the weight and bias parameters for a softmax regression classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim - integer; The number of features in the input data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    W - A numpy array of shape (dim, 3) containing \n",
    "        the weight parameters for the softmax regression classifier.\n",
    "    b - A numpy array of shape (1, 3) containing \n",
    "        the bias parameters for the softmax regression classifier.\n",
    "\n",
    "    \"\"\"\n",
    "    W = np.zeros((dim, 3))\n",
    "    b = np.zeros((1, 3))\n",
    "    \n",
    "    return W, b\n",
    "\n",
    "def propagate(X, Y, W, b):\n",
    "    \"\"\"\n",
    "    Computes forward propagation and backward propagation for a softmax regression model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X - numpy array of shape (m, n)\n",
    "        The input data, where m is the number of samples and n is the number of features.\n",
    "    Y - numpy array of shape (m, c)\n",
    "        The one-hot encoded target labels, where c is the number of classes.\n",
    "    W - numpy array of shape (n, c)\n",
    "        The weight matrix for the linear transformation.\n",
    "    b - numpy array of shape (1, c)\n",
    "        The bias vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dW - numpy array of shape (n, c)\n",
    "        The gradient of the cost function with respect to W.\n",
    "    db - numpy array of shape (1, c)\n",
    "        The gradient of the cost function with respect to b.\n",
    "    cost - float\n",
    "        The value of the cost function.\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    A = softmax(np.dot(X, W) + b)\n",
    "    cost = -np.mean(np.sum(Y * np.log(A), axis=1))\n",
    "    dZ = A - Y\n",
    "    dW = (1 / m) * np.dot(X.T, dZ)\n",
    "    db = (1 / m) * np.sum(dZ, axis=0)\n",
    "\n",
    "    return dW, db, cost\n",
    "\n",
    "def optimize(X, Y, W, b, num_iter, lr):\n",
    "    \"\"\"\n",
    "    Performs gradient descent optimization to minimize the cross-entropy loss\n",
    "    between the predicted and actual class probabilities.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X - numpy array of shape (n_samples, n_features)\n",
    "        The input data.\n",
    "    Y - numpy array of shape (n_samples, n_classes)\n",
    "        The one-hot encoded target labels.\n",
    "    W - numpy array of shape (n_features, n_classes)\n",
    "        The weight matrix of the softmax regression classifier.\n",
    "    b - numpy array of shape (1, n_classes)\n",
    "        The bias vector of the softmax regression classifier.\n",
    "    num_iter - int\n",
    "        The number of iterations to run the optimization algorithm.\n",
    "    lr - float\n",
    "        The learning rate, which controls the step size of the parameter updates.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    W - numpy array of shape (n_features, n_classes)\n",
    "        The optimized weight matrix.\n",
    "    b - numpy array of shape (1, n_classes)\n",
    "        The optimized bias vector.\n",
    "    costs - list\n",
    "        A list of the cross-entropy losses at every 100 iterations of the\n",
    "        optimization algorithm.\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    for i in range(num_iter):\n",
    "        dW, db, cost = propagate(X, Y, W, b)\n",
    "        W -= lr * dW\n",
    "        b -= lr * db\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "    return W, b, costs\n",
    "\n",
    "def predict(X, W, b):\n",
    "    \"\"\"\n",
    "    Predicts the class label for each sample in X, based on the learned parameters W and b.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X - numpy array of shape (n_samples, n_features)\n",
    "        The input data.\n",
    "\n",
    "    W - numpy array of shape (n_features, n_classes)\n",
    "        The learned weights for the linear transformation.\n",
    "\n",
    "    b - numpy array of shape (1, n_classes)\n",
    "        The learned bias terms for the linear transformation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions - numpy array of shape (n_samples,)\n",
    "        The predicted class label for each sample in X, as an integer between 0 and (n_classes - 1).\n",
    "    \"\"\"\n",
    "    A = softmax(np.dot(X, W) + b)\n",
    "    return np.argmax(A, axis=1)\n",
    "\n",
    "def bagging(train_features, train_labels, test_features, test_labels, num_bagging):\n",
    "    \"\"\"\n",
    "    Applies bagging to train a model on different subsets of the training data\n",
    "    and then aggregates their predictions to make a final prediction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_features - numpy array of shape (n_train_samples, n_features)\n",
    "        The features of the training data.\n",
    "    train_labels - numpy array of shape (n_train_samples, n_classes)\n",
    "        The one-hot encoded labels of the training data.\n",
    "    test_features - numpy array of shape (n_test_samples, n_features)\n",
    "        The features of the test data.\n",
    "    test_labels - numpy array of shape (n_test_samples, n_classes)\n",
    "        The one-hot encoded labels of the test data.\n",
    "    num_bagging - int\n",
    "        The number of subsets to create and train models on.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pred - numpy array of shape (n_test_samples,)\n",
    "        The predicted labels for the test data.\n",
    "    \"\"\"\n",
    "    bagging_pred = np.zeros((test_labels.shape[0], num_bagging))\n",
    "    for i in range(num_bagging):\n",
    "        idx = np.random.choice(train_features.shape[0], train_features.shape[0])\n",
    "        X_bag, y_bag = train_features[idx], train_labels[idx]\n",
    "        W, b = initialize_params(train_features.shape[1])\n",
    "        W, b, _ = optimize(X_bag, y_bag, W, b, num_iter=10000, lr=0.1)\n",
    "        bagging_pred[:, i] = predict(test_features, W, b)\n",
    "    \n",
    "    pred = np.argmax(np.apply_along_axis(lambda x: np.bincount(x.astype('int64'), minlength=3), axis=1, arr=bagging_pred), axis=1)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "189c014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the bagging algorithm for single, 10, 50 and 100 cases\n",
    "pred_1 = bagging(train_features, train_labels, test_features, test_features, num_bagging=1)\n",
    "pred_10 = bagging(train_features, train_labels, test_features, test_features, num_bagging=10)\n",
    "pred_50 = bagging(train_features, train_labels, test_features, test_features, num_bagging=50)\n",
    "pred_100 = bagging(train_features, train_labels, test_features, test_features, num_bagging=100)\n",
    "\n",
    "## convert actual label to normal encoded labels for testing\n",
    "pred_actual = np.array([np.argmax(i) for i in test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93670c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Classifier Accuracy: 63.33%\n",
      "Bagging 10 Accuracy: 76.67%\n",
      "Bagging 50 Accuracy: 80.0%\n",
      "Bagging 100 Accuracy: 80.0%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Single Classifier Accuracy: {round(100*np.mean(pred_1==pred_actual),2)}%\")\n",
    "print(f\"Bagging 10 Accuracy: {round(100*np.mean(pred_10==pred_actual),2)}%\")\n",
    "print(f\"Bagging 50 Accuracy: {round(100*np.mean(pred_50==pred_actual),2)}%\")\n",
    "print(f\"Bagging 100 Accuracy: {round(100*np.mean(pred_100==pred_actual),2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d663bc7",
   "metadata": {},
   "source": [
    "## Observations -\n",
    "The results obtained show that the bagging algorithm improves the performance of a single classifier. As we can see from the results, the accuracy of a single classifier is 63.33%, while the accuracy of bagging with 10, 50, and 100 classifiers is 76.67%, 80.0%, and 80.0%, respectively. \n",
    "\n",
    "This increase in accuracy can be explained by the fact that bagging helps to reduce overfitting by training each classifier on a different subset of the data. By combining the predictions of multiple classifiers, we can reduce the variance of the model and improve its generalization performance on unseen data.\n",
    "\n",
    "However, it's important to note that the results obtained may vary depending on the dataset and the choice of hyperparameters. In practice, it's recommended to perform a thorough hyperparameter tuning to achieve the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19342db9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4da621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40b6cb91",
   "metadata": {},
   "source": [
    "## ------------------------- Question 2 (A,B) -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3718262f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- Question 2 (A,B) -------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------- Question 2 (A,B) -------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa7f5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "## softmax function already defined above\n",
    "\n",
    "def cross_entropy_loss(y_hat, y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss between the predicted probability \n",
    "    distribution and the true distribution of labels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_hat - numpy array of shape (m, k)\n",
    "        Predicted probability distribution, where m is the number of samples \n",
    "        and k is the number of classes.\n",
    "    y - numpy array of shape (m, k)\n",
    "        True distribution of labels in one-hot encoding.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    loss - float\n",
    "        Cross-entropy loss value between y_hat and y.\n",
    "    \"\"\"\n",
    "    m = y_hat.shape[0]\n",
    "    loss = -np.sum(y * np.log(y_hat)) / m\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    \"\"\"\n",
    "    Converts the input array of labels to one-hot encoded matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y - numpy array of shape (n_samples,)\n",
    "        The input array of integer labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    one_hot - numpy array of shape (n_samples, n_classes)\n",
    "        The one-hot encoded matrix, where each row represents a sample and\n",
    "        each column represents a class. The column corresponding to the \n",
    "        class of the sample contains 1, and all other columns contain 0.\n",
    "    \"\"\"\n",
    "    n_values = np.max(y) + 1\n",
    "    \n",
    "    return np.eye(n_values)[y]\n",
    "\n",
    "def softmax_regression(X, y, num_classes, num_iterations, learning_rate):\n",
    "    \"\"\"\n",
    "    Trains a softmax regression model on the given input data X and labels y \n",
    "    to classify the samples into the specified number of classes using \n",
    "    stochastic gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array of shape (n_samples, n_features)\n",
    "        The input features to be classified.\n",
    "    y : numpy array of shape (n_samples,)\n",
    "        The class labels corresponding to each input sample.\n",
    "    num_classes : int\n",
    "        The number of classes to classify the input samples into.\n",
    "    num_iterations : int\n",
    "        The number of iterations to train the model for.\n",
    "    learning_rate : float\n",
    "        The learning rate for the gradient descent optimization.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    W : numpy array of shape (n_features, num_classes)\n",
    "        The learned weight matrix for the softmax regression model.\n",
    "    b : numpy array of shape (1, num_classes)\n",
    "        The learned bias vector for the softmax regression model.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    W = np.zeros((n, num_classes))\n",
    "    b = np.zeros((1, num_classes))\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        Z = np.dot(X, W) + b\n",
    "        A = softmax(Z)\n",
    "        dZ = A - one_hot_encode(y)\n",
    "        dW = np.dot(X.T, dZ)\n",
    "        db = np.sum(dZ, axis=0, keepdims=True)\n",
    "        W -= learning_rate * dW\n",
    "        b -= learning_rate * db\n",
    "\n",
    "        if (i+1) % 1000 == 0:\n",
    "            loss = cross_entropy_loss(A, one_hot_encode(y))\n",
    "            print(\"Iteration %d, loss: %f\" % (i+1, loss))\n",
    "\n",
    "    return W, b\n",
    "\n",
    "def adaboost(X, y, num_classes, num_iterations, learning_rate, num_boosts):\n",
    "    \"\"\"\n",
    "    Trains an AdaBoost classifier by iteratively boosting multiple instances\n",
    "    of a softmax regression classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array of shape (n_samples, n_features)\n",
    "        The input feature matrix.\n",
    "\n",
    "    y : numpy array of shape (n_samples,)\n",
    "        The input array of integer labels.\n",
    "\n",
    "    num_classes : int\n",
    "        The number of classes in the classification problem.\n",
    "\n",
    "    num_iterations : int\n",
    "        The number of iterations to train the softmax regression classifier.\n",
    "\n",
    "    learning_rate : float\n",
    "        The learning rate used in the softmax regression classifier.\n",
    "\n",
    "    num_boosts : int\n",
    "        The number of iterations to boost the softmax regression classifier.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    classifiers : list of tuples\n",
    "        A list of tuples, where each tuple contains the weights and bias term\n",
    "        of a trained softmax regression classifier.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    weights = np.ones((m, 1)) / m\n",
    "    classifiers = []\n",
    "\n",
    "    for t in range(num_boosts):\n",
    "        print(\"Boosting round %d\" % (t+1))\n",
    "        W, b = softmax_regression(X, y, num_classes, num_iterations, learning_rate)\n",
    "        classifiers.append((W, b))\n",
    "\n",
    "        y_hat = softmax(np.dot(X, W) + b)\n",
    "        error = np.sum((y_hat.argmax(axis=1) != y)) / m\n",
    "        alpha = 0.5 * np.log((1 - error) / error)\n",
    "\n",
    "        y_pred = y_hat.argmax(axis=1)\n",
    "        y_pred[y_pred != y] = -1\n",
    "        y_pred[y_pred == y] = 1\n",
    "\n",
    "        weights *= np.exp(-alpha * y_pred.reshape(m, 1))\n",
    "        weights /= np.sum(weights)\n",
    "\n",
    "    return classifiers\n",
    "\n",
    "def predict(X, classifiers):\n",
    "    \"\"\"\n",
    "    Takes in a set of input data samples and a set of trained classifiers \n",
    "    as input and returns the predicted class labels for each sample.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: numpy array of shape (n_samples, n_features)\n",
    "        The input data samples to be classified.\n",
    "    \n",
    "    classifiers: list of tuples, each tuple containing:\n",
    "    \n",
    "    W: numpy array of shape (n_features, n_classes)\n",
    "        The weights learned by the softmax regression classifier.\n",
    "    \n",
    "    b: numpy array of shape (1, n_classes)\n",
    "        The biases learned by the softmax regression classifier.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y_hat: numpy array of shape (n_samples,)\n",
    "        The predicted class labels for each input data sample in X.\n",
    "    \"\"\"\n",
    "    y_hat = np.zeros((X.shape[0], classifiers[0][0].shape[1]))\n",
    "\n",
    "    for W, b in classifiers:\n",
    "        y_hat += softmax(np.dot(X, W) + b)\n",
    "\n",
    "    return y_hat.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3199f746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000, loss: 1.619566\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array([np.argmax(i) for i in train_labels])\n",
    "y_test = np.array([np.argmax(i) for i in test_labels])\n",
    "\n",
    "# Train softmax regression classifier\n",
    "W, b = softmax_regression(train_features, \n",
    "                          np.array([np.argmax(i) for i in train_labels]), \n",
    "                          num_classes=3, \n",
    "                          num_iterations=1000, \n",
    "                          learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92d5d15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting round 1\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 2\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 3\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 4\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 5\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 6\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 7\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 8\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 9\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 10\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 1\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 2\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 3\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 4\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 5\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 6\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 7\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 8\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 9\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 10\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 11\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 12\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 13\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 14\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 15\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 17\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 18\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 19\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 20\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 21\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 22\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 23\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 24\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 25\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 1\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 2\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 3\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 4\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 5\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 6\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 7\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 8\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 9\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 10\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 11\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 12\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 13\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 14\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 15\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 16\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 17\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 18\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 19\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 20\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 21\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 22\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 23\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 24\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 25\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 26\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 27\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 28\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 29\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 30\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 31\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 32\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 33\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 34\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 35\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 36\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 37\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 38\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 39\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 40\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 41\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 42\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 43\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 44\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 45\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 46\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 47\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 48\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 49\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n",
      "Boosting round 50\n",
      "Iteration 1000, loss: 5.779207\n",
      "Iteration 2000, loss: 18.850148\n",
      "Iteration 3000, loss: 2.192127\n",
      "Iteration 4000, loss: 2.801846\n",
      "Iteration 5000, loss: 2.870549\n",
      "Iteration 6000, loss: 2.871178\n",
      "Iteration 7000, loss: 2.858433\n",
      "Iteration 8000, loss: 2.840796\n",
      "Iteration 9000, loss: 2.819665\n",
      "Iteration 10000, loss: 2.795823\n"
     ]
    }
   ],
   "source": [
    "# Evaluate single classifier\n",
    "y_pred = predict(test_features, [(W, b)])\n",
    "error_rate_single = np.sum((y_pred != y_test)) / y_test.shape[0]\n",
    "\n",
    "# Train AdaBoost ensemble\n",
    "classifiers_10 = adaboost(train_features, \n",
    "                          y_train, \n",
    "                          num_classes=3, \n",
    "                          num_iterations=10000, \n",
    "                          learning_rate=0.1, \n",
    "                          num_boosts=10)\n",
    "\n",
    "classifiers_25 = adaboost(train_features, \n",
    "                          y_train, \n",
    "                          num_classes=3, \n",
    "                          num_iterations=10000, \n",
    "                          learning_rate=0.1, \n",
    "                          num_boosts=25)\n",
    "\n",
    "classifiers_50 = adaboost(train_features, \n",
    "                          y_train, \n",
    "                          num_classes=3, \n",
    "                          num_iterations=10000, \n",
    "                          learning_rate=0.1, \n",
    "                          num_boosts=50)\n",
    "\n",
    "# Evaluate AdaBoost ensembles\n",
    "y_pred_10 = predict(test_features, classifiers_10)\n",
    "error_rate_10 = np.sum((y_pred_10 != y_test)) / y_test.shape[0]\n",
    "\n",
    "y_pred_25 = predict(test_features, classifiers_25)\n",
    "error_rate_25 = np.sum((y_pred_25 != y_test)) / y_test.shape[0]\n",
    "\n",
    "y_pred_50 = predict(test_features, classifiers_50)\n",
    "error_rate_50 = np.sum((y_pred_50 != y_test)) / y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84e23374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Classifier Error Rate: 23.33%\n",
      "Bagging 10 Accuracy: 3.33%\n",
      "Bagging 50 Accuracy: 3.33%\n",
      "Bagging 100 Accuracy: 3.33%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Single Classifier Error Rate: {round(100*error_rate_single,2)}%\")\n",
    "print(f\"Bagging 10 Accuracy: {round(100*error_rate_10,2)}%\")\n",
    "print(f\"Bagging 50 Accuracy: {round(100*error_rate_25,2)}%\")\n",
    "print(f\"Bagging 100 Accuracy: {round(100*error_rate_50,2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df61f20",
   "metadata": {},
   "source": [
    "## Observations -\n",
    "The results of the evaluation show that the AdaBoost ensembles significantly outperformed the single classifier. The single classifier had an error rate of 23.3%, while the AdaBoost ensembles with 10, 25, and 50 boosting rounds all had an error rate of only 3.3%. This is a substantial improvement in accuracy, indicating that AdaBoost is a powerful technique for improving the performance of machine learning models.\n",
    "\n",
    "Additionally, we can see that the performance of the AdaBoost ensembles does not seem to improve beyond 25 boosting rounds, as the error rate remains constant at 3.3% for 25 and 50 boosting rounds. This suggests that further boosting may not be necessary and could potentially lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906a2381",
   "metadata": {},
   "source": [
    "## ------------------------- Question 3 (A,B) -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c91ccc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- Question 3 (A,B) -------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------- Question 3 (A,B) -------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e6c8e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Means Clustering (K=3) overall accuracy: 0.433333\n",
      "K-Means Clustering (K=6) overall accuracy: 0.458333\n",
      "K-Means Clustering (K=9) overall accuracy: 0.575000\n",
      "\n",
      "\n",
      "K-Means Clustering (K=3) overall accuracy: 0.683333 (scaled)\n",
      "K-Means Clustering (K=6) overall accuracy: 0.741667 (scaled)\n",
      "K-Means Clustering (K=9) overall accuracy: 0.758333 (scaled)\n"
     ]
    }
   ],
   "source": [
    "def k_means_clustering(X, k, num_iterations=100):\n",
    "    \"\"\"\n",
    "    Implements the k-means clustering algorithm on a given dataset to identify k clusters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array of shape (n_samples, n_features)\n",
    "        The input data matrix.\n",
    "\n",
    "    k : int\n",
    "        The number of clusters to identify.\n",
    "\n",
    "    num_iterations : int, optional (default=100)\n",
    "        The maximum number of iterations to run the algorithm for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    centroids : numpy array of shape (k, n_features)\n",
    "        The final centroids of the k clusters.\n",
    "\n",
    "    cluster_assignments : numpy array of shape (n_samples,)\n",
    "        An array containing the cluster assignments of each point in X.\n",
    "    \"\"\"\n",
    "    # Initialize centroids randomly\n",
    "    centroids = X[np.random.choice(X.shape[0], k, replace=False), :]\n",
    "\n",
    "    # Iterate until convergence or maximum number of iterations is reached\n",
    "    for i in range(num_iterations):\n",
    "        # Assign each point to the nearest centroid\n",
    "        distances = np.linalg.norm(X[:, np.newaxis, :] - centroids, axis=2)\n",
    "        cluster_assignments = np.argmin(distances, axis=1)\n",
    "\n",
    "        # Update centroids to be the mean of the points in each cluster\n",
    "        for j in range(k):\n",
    "            mask = (cluster_assignments == j)\n",
    "            if np.any(mask):\n",
    "                centroids[j] = np.mean(X[mask, :], axis=0)\n",
    "\n",
    "    return centroids, cluster_assignments\n",
    "\n",
    "def compute_cluster_accuracy(cluster_assignments, true_labels, k):\n",
    "    accuracies = []\n",
    "    for j in range(k):\n",
    "        mask = (cluster_assignments == j)\n",
    "        if np.any(mask):\n",
    "            counts = np.bincount(true_labels[mask])\n",
    "            accuracy = np.max(counts) / np.sum(counts)\n",
    "            accuracies.append(accuracy * np.sum(mask) / true_labels.shape[0])\n",
    "    return np.sum(accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f7d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scale the data\n",
    "features_scaled = (features - np.mean(features, axis=0)) / np.std(features, axis=0)\n",
    "\n",
    "## Apply K-Means Clustering with K=3, 6, and 9\n",
    "for k in [3, 6, 9]:\n",
    "    centroids, cluster_assignments = k_means_clustering(features, k)\n",
    "    accuracy = compute_cluster_accuracy(cluster_assignments, \n",
    "                                        np.array([0 if i=='Plastic' else 1 if i=='Ceramic' else 2 for i in labels]), \n",
    "                                        k)\n",
    "    print(\"K-Means Clustering (K=%d) overall accuracy: %f\" % (k, accuracy))\n",
    "    \n",
    "print('\\n')\n",
    "## use scaled data to compare performance\n",
    "## Apply K-Means Clustering with K=3, 6, and 9\n",
    "for k in [3, 6, 9]:\n",
    "    centroids, cluster_assignments = k_means_clustering(features_scaled, k)\n",
    "    accuracy = compute_cluster_accuracy(cluster_assignments, \n",
    "                                        np.array([0 if i=='Plastic' else 1 if i=='Ceramic' else 2 for i in labels]), \n",
    "                                        k)\n",
    "    print(\"K-Means Clustering (K=%d) overall accuracy: %f (scaled)\" % (k, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a520ce12",
   "metadata": {},
   "source": [
    "## Observations -\n",
    "### Before scaling the features -\n",
    "The results of the K-Means clustering algorithm show that the accuracy increases with an increase in the number of clusters, as expected. With K=3, the overall accuracy of the algorithm is 43.33%, which is not very high. This is likely due to the fact that there are three distinct material types in the dataset, which may not be easily separable into just three clusters.\n",
    "\n",
    "With K=6, the overall accuracy increases to 47.5%. This suggests that some of the overlap between the different material types is being captured by the algorithm, but there is still some confusion between the different clusters.\n",
    "\n",
    "Finally, with K=9, the overall accuracy increases further to 55.8%. This suggests that the additional clusters are helping to better capture the different material types and reduce the confusion between them.\n",
    "\n",
    "Overall, the results suggest that K-Means clustering can be effective at identifying the different material types in the dataset, but that a larger number of clusters may be needed to achieve high accuracy. Additionally, it's worth noting that the accuracy of K-Means clustering is limited by the intrinsic separability of the data, which may not be perfect in all cases.\n",
    "\n",
    "### After scaling the features -  \n",
    "Scaling the data had a significant impact on the performance of K-Means clustering. The overall accuracy increased for all values of K, which suggests that scaling improved the clustering results.\n",
    "\n",
    "In particular, the accuracy of the K-Means clustering (K=9) increased from 0.558 to 0.758 after scaling. This is a substantial improvement and indicates that the clusters are better aligned with the true labels.\n",
    "\n",
    "Scaling is an important preprocessing step for many machine learning algorithms, as it can help improve the performance and stability of the models. In this case, scaling helped K-Means clustering to better capture the structure of the data and produce more accurate clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6040e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f53c33f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c841d912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
