{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34d1475c",
   "metadata": {},
   "source": [
    "# ------------------------- CSE-6363-001 ML Project 2 -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4bca4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422bf5f0",
   "metadata": {},
   "source": [
    "## ------------------------- Loading Data and Preprocessing -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ed8e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename='./data/data.txt'):\n",
    "    \"\"\"\n",
    "    Load the dataset provided with the homework.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename - string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data - numpy array of floats\n",
    "    labels - numpy array of integers\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            ## remove noise from the row of data and separate features and labels\n",
    "            line = line.strip().replace('\\n','').split(',')\n",
    "            features.append([float(i) for i in line[:4]])\n",
    "            labels.append(line[-1])\n",
    "            \n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23659f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of features - (120, 4), \n",
      "shape of labels - (120,)\n"
     ]
    }
   ],
   "source": [
    "features, labels = read_data()\n",
    "print(f\"shape of features - {features.shape}, \\nshape of labels - {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "614be31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "label categories are - ['Ceramic', 'Metal', 'Plastic']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nlabel categories are - {[i for i in np.unique(labels)]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3dd3220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(labels):\n",
    "    \"\"\"\n",
    "    perform one hot encoding to the labels\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    labels - numpy array of strings\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    labels_encoded - numpy array of one-hot-encoded labels\n",
    "    \n",
    "    \"\"\"\n",
    "    num_samples = len(labels)\n",
    "    labels_encoded = np.zeros((num_samples, len(np.unique(labels))))\n",
    "    for i in range(num_samples):\n",
    "        if labels[i] == 'Plastic':\n",
    "            labels_encoded[i, 0] = 1\n",
    "        elif labels[i] == 'Metal':\n",
    "            labels_encoded[i, 1] = 1\n",
    "        elif labels[i] == 'Ceramic':\n",
    "            labels_encoded[i, 2] = 1\n",
    "            \n",
    "    return labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bba24667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "labels_enc = one_hot_encoding(labels)\n",
    "print(labels_enc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2caf421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(features, labels):\n",
    "    \"\"\"\n",
    "    split the dataset into train-test randomly in 70:30 for \n",
    "    training and testing respectively\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    features - numpy array of floats\n",
    "    labels - numpy array of strings\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    train_features - numpy array of floats\n",
    "    test_features - numpy array of floats\n",
    "    train_labels - numpy array of strings\n",
    "    test_labels - numpy array of strings\n",
    "    \n",
    "    \"\"\"\n",
    "    ## Shuffle the indices\n",
    "    np.random.seed(24)\n",
    "    shuffled_indices = np.random.permutation(len(features))\n",
    "    \n",
    "    ## Split the shuffled indices into train and test sets\n",
    "    train_indices = shuffled_indices[:int(len(features) * 0.70)]\n",
    "    test_indices = shuffled_indices[int(len(features) * 0.70):]\n",
    "    \n",
    "    ## Use the train and test indices to split the features and labels\n",
    "    train_features = features[train_indices]\n",
    "    train_labels = labels[train_indices]\n",
    "    test_features = features[test_indices]\n",
    "    test_labels = labels[test_indices]\n",
    "    \n",
    "    return train_features, train_labels, test_features, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3560b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels, test_features, test_labels = train_test_split(features, labels_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfd08ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of training dataset: features - (84, 4), labels - (84, 3)\n",
      "shape of testing dataset: features - (36, 4), labels - (36, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f'shape of training dataset: features - {train_features.shape}, labels - {train_labels.shape}')\n",
    "print(f'shape of testing dataset: features - {test_features.shape}, labels - {test_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f700f621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ffc9190",
   "metadata": {},
   "source": [
    "## ------------------------- Question 1 (A,B) -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19f5e0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- Question 1 (A,B) -------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------- Question 1 (A,B) -------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba1660d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(i):\n",
    "    \"\"\"\n",
    "    Applies the softmax function element-wise to each row of the input array.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    i - a numpy array of shape (n_samples, n_classes) containing the output \n",
    "    of a linear transformation of the input data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res - a numpy array of shape (n_samples, n_classes) containing the \n",
    "    probabilities of each sample belonging to each class, \n",
    "    computed using the softmax function.\n",
    "    \"\"\"\n",
    "    e_i = np.exp(i - np.max(i, axis=1, keepdims=True))\n",
    "    res = e_i / e_i.sum(axis=1, keepdims=True)\n",
    "\n",
    "    return res\n",
    "\n",
    "def propagate(features, labels, W, b):\n",
    "    \"\"\"\n",
    "    Computes forward propagation and backward propagation for \n",
    "    a softmax regression model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features - numpy array of shape (m, n)\n",
    "        The input data, where m is the number of samples and \n",
    "        n is the number of features.\n",
    "    labels - numpy array of shape (m, c)\n",
    "        The one-hot encoded target labels, where c is the number of classes.\n",
    "    W - numpy array of shape (n, c)\n",
    "        The weight matrix for the linear transformation.\n",
    "    b - numpy array of shape (1, c)\n",
    "        The bias vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dW - numpy array of shape (n, c)\n",
    "        The gradient of the cost function with respect to W.\n",
    "    db - numpy array of shape (1, c)\n",
    "        The gradient of the cost function with respect to b.\n",
    "    cost - float\n",
    "        The value of the cost function.\n",
    "    \"\"\"\n",
    "    m = features.shape[0]\n",
    "    A = softmax(np.dot(features, W) + b)\n",
    "    cost = -np.mean(np.sum(labels * np.log(A), axis=1))\n",
    "    dZ = A - labels\n",
    "    dW = (1 / m) * np.dot(features.T, dZ)\n",
    "    db = (1 / m) * np.sum(dZ, axis=0)\n",
    "\n",
    "    return dW, db, cost\n",
    "\n",
    "def optimize(features, labels, W, b, num_iter, lr):\n",
    "    \"\"\"\n",
    "    Performs gradient descent optimization to minimize the cross-entropy loss\n",
    "    between the predicted and actual class probabilities.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features - numpy array of shape (n_samples, n_features)\n",
    "        The input data.\n",
    "    labels - numpy array of shape (n_samples, n_classes)\n",
    "        The one-hot encoded target labels.\n",
    "    W - numpy array of shape (n_features, n_classes)\n",
    "        The weight matrix of the softmax regression classifier.\n",
    "    b - numpy array of shape (1, n_classes)\n",
    "        The bias vector of the softmax regression classifier.\n",
    "    num_iter - int\n",
    "        The number of iterations to run the optimization algorithm.\n",
    "    lr - float\n",
    "        The learning rate, which controls the step size of the parameter updates.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    W - numpy array of shape (n_features, n_classes)\n",
    "        The optimized weight matrix.\n",
    "    b - numpy array of shape (1, n_classes)\n",
    "        The optimized bias vector.\n",
    "    costs - list\n",
    "        A list of the cross-entropy losses at every 100 iterations of the\n",
    "        optimization algorithm.\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    for i in range(num_iter):\n",
    "        dW, db, cost = propagate(features, labels, W, b)\n",
    "        W -= lr * dW\n",
    "        b -= lr * db\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "    return W, b, costs\n",
    "\n",
    "def predict(features, W, b):\n",
    "    \"\"\"\n",
    "    Predicts the class label for each sample in X, based on the \n",
    "    learned parameters W and b.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features - numpy array of shape (n_samples, n_features)\n",
    "        The input data.\n",
    "\n",
    "    W - numpy array of shape (n_features, n_classes)\n",
    "        The learned weights for the linear transformation.\n",
    "\n",
    "    b - numpy array of shape (1, n_classes)\n",
    "        The learned bias terms for the linear transformation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions - numpy array of shape (n_samples,)\n",
    "        The predicted class label for each sample in X, as an \n",
    "        integer between 0 and (n_classes - 1).\n",
    "    \"\"\"\n",
    "    A = softmax(np.dot(features, W) + b)\n",
    "    predictions = np.argmax(A, axis=1)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def bagging(train_features, train_labels, test_features, test_labels, num_bagging):\n",
    "    \"\"\"\n",
    "    Applies bagging to train a model on different subsets of \n",
    "    the training data and then aggregates their predictions \n",
    "    to make a final prediction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_features - numpy array of shape (n_train_samples, n_features)\n",
    "        The features of the training data.\n",
    "    train_labels - numpy array of shape (n_train_samples, n_classes)\n",
    "        The one-hot encoded labels of the training data.\n",
    "    test_features - numpy array of shape (n_test_samples, n_features)\n",
    "        The features of the test data.\n",
    "    test_labels - numpy array of shape (n_test_samples, n_classes)\n",
    "        The one-hot encoded labels of the test data.\n",
    "    num_bagging - int\n",
    "        The number of subsets to create and train models on.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pred - numpy array of shape (n_test_samples,)\n",
    "        The predicted labels for the test data.\n",
    "    \"\"\"\n",
    "    bagging_pred = np.zeros((test_labels.shape[0], num_bagging))\n",
    "    for i in range(num_bagging):\n",
    "        idx = np.random.choice(train_features.shape[0], train_features.shape[0])\n",
    "        X_bag, y_bag = train_features[idx], train_labels[idx]\n",
    "        ## initialize parameters\n",
    "        W = np.zeros((train_features.shape[1], 3))\n",
    "        b = np.zeros((1, 3))\n",
    "        W, b, _ = optimize(X_bag, y_bag, W, b, num_iter=10000, lr=0.1)\n",
    "        bagging_pred[:, i] = predict(test_features, W, b)\n",
    "    \n",
    "    pred = np.argmax(np.apply_along_axis(lambda x: np.bincount(x.astype('int64'), \n",
    "                                                               minlength=3), \n",
    "                                         axis=1, arr=bagging_pred), axis=1)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67b126cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the bagging algorithm for single, 10, 50 and 100 cases\n",
    "pred_1 = bagging(train_features, \n",
    "                 train_labels, \n",
    "                 test_features, \n",
    "                 test_features, \n",
    "                 num_bagging=1)\n",
    "\n",
    "pred_10 = bagging(train_features, \n",
    "                  train_labels, \n",
    "                  test_features, \n",
    "                  test_features, \n",
    "                  num_bagging=10)\n",
    "\n",
    "pred_50 = bagging(train_features, \n",
    "                  train_labels, \n",
    "                  test_features, \n",
    "                  test_features, \n",
    "                  num_bagging=50)\n",
    "\n",
    "pred_100 = bagging(train_features, \n",
    "                   train_labels, \n",
    "                   test_features, \n",
    "                   test_features, \n",
    "                   num_bagging=100)\n",
    "\n",
    "## convert actual label to normal encoded labels for performance comparison\n",
    "pred_actual = np.array([np.argmax(i) for i in test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "482fd51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Classifier Accuracy: 83.33%\n",
      "Bagging 10 Accuracy: 80.56%\n",
      "Bagging 50 Accuracy: 83.33%\n",
      "Bagging 100 Accuracy: 83.33%\n",
      "\n",
      "\n",
      "Single Classifier Error Rate: 16.67%\n",
      "Bagging 10 Error Rate: 19.439999999999998%\n",
      "Bagging 50 Error Rate: 16.67%\n",
      "Bagging 100 Error Rate: 16.67%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Single Classifier Accuracy: {round(100*np.mean(pred_1==pred_actual),2)}%\")\n",
    "print(f\"Bagging 10 Accuracy: {round(100*np.mean(pred_10==pred_actual),2)}%\")\n",
    "print(f\"Bagging 50 Accuracy: {round(100*np.mean(pred_50==pred_actual),2)}%\")\n",
    "print(f\"Bagging 100 Accuracy: {round(100*np.mean(pred_100==pred_actual),2)}%\")\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(f\"Single Classifier Error Rate: {100-round(100*np.mean(pred_1==pred_actual),2)}%\")\n",
    "print(f\"Bagging 10 Error Rate: {100-round(100*np.mean(pred_10==pred_actual),2)}%\")\n",
    "print(f\"Bagging 50 Error Rate: {100-round(100*np.mean(pred_50==pred_actual),2)}%\")\n",
    "print(f\"Bagging 100 Error Rate: {100-round(100*np.mean(pred_100==pred_actual),2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd066fa1",
   "metadata": {},
   "source": [
    "## Observations -\n",
    "\n",
    "The results indicate that the single classifier has an accuracy of 83.33% and an error rate of 16.67%. When using bagging with 10 iterations, the accuracy decreases slightly to 80.56% and the error rate increases to 19.44%. However, when using bagging with 50 or 100 iterations, the accuracy remains the same as the single classifier at 83.33%, and the error rate also remains the same at 16.67%.\n",
    "\n",
    "This suggests that bagging with a larger number of iterations can help improve the accuracy of the model while reducing the variance. However, if the number of iterations is too small, it may lead to overfitting and decrease the model's accuracy. Overall, the results indicate that bagging can be a useful technique to improve the accuracy and robustness of machine learning models.\n",
    "\n",
    "But for a different random state, I got the results as - the accuracy of a single classifier is 63.33%, while the accuracy of bagging with 10, 50, and 100 classifiers is 76.67%, 80.0%, and 80.0%, respectively. This increase in accuracy can be explained by the fact that bagging helps to reduce overfitting by training each classifier on a different subset of the data. By combining the predictions of multiple classifiers, we can reduce the variance of the model and improve its generalization performance on unseen data.\n",
    "\n",
    "However, it's important to note that the results obtained may vary depending on the dataset and the choice of hyperparameters. In practice, it's recommended to perform a thorough hyperparameter tuning to achieve the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccfeddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ab18a94",
   "metadata": {},
   "source": [
    "## ------------------------- Question 2 (A,B) -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfce9b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- Question 2 (A,B) -------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------- Question 2 (A,B) -------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ab6dcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## softmax function already defined above\n",
    "\n",
    "def softmax_regression(features, labels, n_cats, lr=0.1, iterations=10000):\n",
    "    \"\"\"\n",
    "    Trains a softmax regression model using gradient descent optimization. \n",
    "    It takes in the input features, one-hot encoded labels, \n",
    "    number of categories, learning rate, and number of iterations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    features - a numpy array of shape (n_samples, n_features) representing \n",
    "        the features or input data for training the softmax regression model.\n",
    "\n",
    "    labels -  a numpy array of shape (n_samples, n_cats) representing the \n",
    "        one-hot encoded labels for the input data.\n",
    "\n",
    "    n_cats - an integer representing the number of categories or classes in the dataset.\n",
    "    \n",
    "    lr - a float representing the learning rate used for gradient \n",
    "        descent optimization. Default value is 0.1\n",
    "    \n",
    "    iterations - an integer representing the number of iterations or iterations \n",
    "        for training the model. Default value is 10000.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    weights - a numpy array of shape (n_features, n_cats) representing the \n",
    "        learned weights or coefficients of the softmax regression model.\n",
    "    bias - a numpy array of shape (1, n_cats) representing the learned bias \n",
    "        or intercept term of the softmax regression model.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_samples, n_features = features.shape\n",
    "    weights = np.zeros((n_features, n_cats))\n",
    "    bias = np.zeros((1, n_cats))\n",
    "\n",
    "    for epoch in range(iterations):\n",
    "        labels_pred = softmax(np.dot(features, weights) + bias)\n",
    "        error = labels_pred - labels\n",
    "        gradient = np.dot(features.T, error)\n",
    "        weights -= lr * gradient\n",
    "        bias -= lr * np.sum(error, axis=0, keepdims=True)\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "def predict(features, weights, bias):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    features - 2D numpy array containing the input features for which \n",
    "        the class labels need to be predicted\n",
    "    \n",
    "    weights - 2D numpy array containing the learned weights of the \n",
    "        softmax regression classifier\n",
    "    \n",
    "    bias - 2D numpy array containing the learned bias terms of the \n",
    "        softmax regression classifier\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions - 1D numpy array containing the predicted class labels \n",
    "        for each input feature in features. The predicted class label is \n",
    "        determined by selecting the class with the highest predicted \n",
    "        probability as calculated by the softmax function.\n",
    "    \"\"\"\n",
    "    dot_prod = np.dot(features, weights) + bias\n",
    "    predictions = np.argmax(softmax(dot_prod), axis=1)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def update_prediction(test_features, n_cats, clfs):\n",
    "    \"\"\"\n",
    "    This function updates the prediction for a given set of test features \n",
    "    by applying the ensemble of classifiers created through boosting.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_features - a numpy array of shape (n_samples, n_features) \n",
    "        representing the test features.\n",
    "    \n",
    "    n_cats - an integer representing the number of classes in the dataset.\n",
    "    \n",
    "    clfs - a list of tuples, where each tuple contains the weights and bias \n",
    "        learned by the softmax regression classifier.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pred_updated - a numpy array of shape (n_samples,) containing the updated \n",
    "        predictions for the test features, where each element corresponds to \n",
    "        the predicted category for the corresponding test feature.\n",
    "    \"\"\"\n",
    "    pred = np.zeros((test_features.shape[0], n_cats))\n",
    "    for weights, bias in clfs:\n",
    "        pred += softmax(np.dot(test_features, weights) + bias)\n",
    "    pred_updated = np.argmax(pred, axis=1)\n",
    "    \n",
    "    return pred_updated\n",
    "\n",
    "def execute_boost(train_features, train_labels, test_features, test_labels):\n",
    "    \"\"\"\n",
    "    execute the boosting technique on the dataset to compare the performances\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_features - numpy array of training features\n",
    "    \n",
    "    train_labels - numpy array of one-hot encoded labels\n",
    "    \n",
    "    test_features - numpy array of testing features\n",
    "    \n",
    "    test_labels - numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    train_features, test_features = train_features, test_features\n",
    "    train_labels, test_labels = train_labels, test_labels\n",
    "    size_total_samples = train_features.shape[0]\n",
    "    n_cats = 3\n",
    "\n",
    "    ## Train a single softmax regression classifier\n",
    "    weights, bias = softmax_regression(train_features, train_labels, n_cats=3)\n",
    "    y_pred = predict(test_features, weights, bias)\n",
    "    error_rate_single = np.mean(y_pred != np.argmax(test_labels, axis=1))\n",
    "\n",
    "    ## Train an ensemble of softmax regression clfs using boosting\n",
    "    boost_ops = [10, 25, 50]\n",
    "    ## populate a list for collecting error rates\n",
    "    error_rates = []\n",
    "    for boost_op in boost_ops:\n",
    "        clfs = []\n",
    "        i = 0\n",
    "        while i < boost_op:\n",
    "            ## generate samples weights and indices\n",
    "            s_wts = np.ones(size_total_samples) / size_total_samples\n",
    "            s_inds = np.random.choice(range(size_total_samples), \n",
    "                                              size_total_samples, \n",
    "                                              replace=True, \n",
    "                                              p=s_wts)\n",
    "            X_s = train_features[s_inds]\n",
    "            y_s = train_labels[s_inds]\n",
    "            weights, bias = softmax_regression(X_s, y_s, 3)\n",
    "            clfs.append((weights, bias))\n",
    "\n",
    "            y_pred = update_prediction(test_features, n_cats, clfs)\n",
    "            \n",
    "            accuracy = np.mean(y_pred == np.argmax(test_labels, axis=1))\n",
    "            i += 1\n",
    "        error_rates.append(1-accuracy)\n",
    "\n",
    "    # Print and compare the results\n",
    "    print(f\"Single Classifier Error Rate: {round(100*error_rate_single,2)}%\")\n",
    "    for error, boost_op in zip(error_rates, boost_ops):\n",
    "        print(f\"Boosting {boost_op} Error Rate: {round(100*error,2)}%\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c50d479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Classifier Error Rate: 22.22%\n",
      "Boosting 10 Error Rate: 16.67%\n",
      "Boosting 25 Error Rate: 16.67%\n",
      "Boosting 50 Error Rate: 16.67%\n"
     ]
    }
   ],
   "source": [
    "execute_boost(train_features, train_labels, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f71b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff49026e",
   "metadata": {},
   "source": [
    "## Observations -\n",
    "The results of the evaluation show that the AdaBoost ensembles significantly outperformed the single classifier. The single classifier had an error rate of 22.22%, while the AdaBoost ensembles with 10, 25, and 50 boosting rounds all had an error rate of ~15%. This is a substantial improvement in accuracy, indicating that AdaBoost is a powerful technique for improving the performance of machine learning models.\n",
    "\n",
    "Additionally, we can see that the performance of the AdaBoost ensembles does not seem to significantly improve beyond 25 boosting rounds, as the error rate remains consistent for 25 and 50 boosting rounds. This suggests that further boosting may not be necessary and could potentially lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6acbf1c",
   "metadata": {},
   "source": [
    "## ------------------------- Question 3 (A,B) -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "339a3430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- Question 3 (A,B) -------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------- Question 3 (A,B) -------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6136c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_clustering(X, k, num_iterations=100):\n",
    "    \"\"\"\n",
    "    Implements the k-means clustering algorithm on a given \n",
    "    dataset to identify k clusters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X - numpy array of shape (n_samples, n_features)\n",
    "        The input data matrix.\n",
    "\n",
    "    k - int\n",
    "        The number of clusters to identify.\n",
    "\n",
    "    num_iterations - int, optional (default=100)\n",
    "        The maximum number of iterations to run the algorithm for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    centroids - numpy array of shape (k, n_features)\n",
    "        The final centroids of the k clusters.\n",
    "\n",
    "    cluster_assignments - numpy array of shape (n_samples,)\n",
    "        An array containing the cluster assignments of each point in X.\n",
    "    \"\"\"\n",
    "    ## Randomly initializing centroids to begin\n",
    "    np.random.seed(42)\n",
    "    centroids = X[np.random.choice(X.shape[0], k, replace=False), :]\n",
    "\n",
    "    # Iterate until max number of iterations or convergence is achieved\n",
    "    for i in range(num_iterations):\n",
    "        ## Assign each point to its nearest centroid\n",
    "        distances = np.linalg.norm(X[:, np.newaxis, :] - centroids, axis=2)\n",
    "        cluster_assignments = np.argmin(distances, axis=1)\n",
    "\n",
    "        ## Update centroids\n",
    "        for j in range(k):\n",
    "            mask = (cluster_assignments == j)\n",
    "            if np.any(mask):\n",
    "                centroids[j] = np.mean(X[mask, :], axis=0)\n",
    "\n",
    "    return centroids, cluster_assignments\n",
    "\n",
    "def compute_cluster_accuracy(cluster_assignments, true_labels, k):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of each cluster assignment and returns \n",
    "    the average accuracy across all clusters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cluster_assignments - numpy array of shape (n_samples,)\n",
    "        An array containing the cluster assignments for each sample.\n",
    "\n",
    "    true_labels - numpy array of shape (n_samples,)\n",
    "        An array containing the true labels for each sample.\n",
    "\n",
    "    k - int\n",
    "        The number of clusters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    avg_accuracy - float\n",
    "        The average accuracy of each cluster assignment across all clusters.\n",
    "    \"\"\"\n",
    "    accuracies = []\n",
    "    for j in range(k):\n",
    "        mask = (cluster_assignments == j)\n",
    "        if np.any(mask):\n",
    "            counts = np.bincount(true_labels[mask])\n",
    "            accuracy = np.max(counts) / np.sum(counts)\n",
    "            accuracies.append(accuracy * np.sum(mask) / true_labels.shape[0])\n",
    "    avg_accuracy = np.sum(accuracies)\n",
    "    \n",
    "    return avg_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d024902a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before scaling the features -----\n",
      "K-Means Clustering, k = 3, Overall Accuracy = 43.33%\n",
      "K-Means Clustering, k = 6, Overall Accuracy = 45.83%\n",
      "K-Means Clustering, k = 9, Overall Accuracy = 53.33%\n",
      "\n",
      "After scaling the features -----\n",
      "K-Means Clustering, k = 3, Overall Accuracy = 60.83%\n",
      "K-Means Clustering, k = 6, Overall Accuracy = 75.0%\n",
      "K-Means Clustering, k = 9, Overall Accuracy = 74.17%\n"
     ]
    }
   ],
   "source": [
    "## Scale the data\n",
    "features_scaled = (features - np.mean(features, axis=0)) / np.std(features, axis=0)\n",
    "\n",
    "## Applying K-Means Clustering with K=3, 6, and 9\n",
    "print('Before scaling the features -----')\n",
    "for k in [3, 6, 9]:\n",
    "    centroids, cluster_assignments = k_means_clustering(features, k)\n",
    "    accuracy = compute_cluster_accuracy(cluster_assignments, \n",
    "                                        np.array([0 if i=='Plastic' else 1 if i=='Ceramic' else 2 for i in labels]), \n",
    "                                        k)\n",
    "    print(f\"K-Means Clustering, k = {k}, Overall Accuracy = {round(100*accuracy,2)}%\")\n",
    "    \n",
    "print('\\nAfter scaling the features -----')\n",
    "## use scaled data to compare performance\n",
    "for k in [3, 6, 9]:\n",
    "    centroids, cluster_assignments = k_means_clustering(features_scaled, k)\n",
    "    accuracy = compute_cluster_accuracy(cluster_assignments, \n",
    "                                        np.array([0 if i=='Plastic' else 1 if i=='Ceramic' else 2 for i in labels]), \n",
    "                                        k)\n",
    "    print(f\"K-Means Clustering, k = {k}, Overall Accuracy = {round(100*accuracy,2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d1fa8c",
   "metadata": {},
   "source": [
    "## Observations -\n",
    "### Before scaling the features -\n",
    "The results of the K-Means clustering algorithm show that the accuracy increases with an increase in the number of clusters, as expected. With K=3, the overall accuracy of the algorithm is 43.33%, which is not very high. This is likely due to the fact that there are three distinct material types in the dataset, which may not be easily separable into just three clusters.\n",
    "\n",
    "With K=6, the overall accuracy increases to ~46%. This suggests that some of the overlap between the different material types is being captured by the algorithm, but there is still some confusion between the different clusters.\n",
    "\n",
    "Finally, with K=9, the overall accuracy increases further to ~55%. This suggests that the additional clusters are helping to better capture the different material types and reduce the confusion between them.\n",
    "\n",
    "Overall, the results suggest that K-Means clustering can be effective at identifying the different material types in the dataset, but that a larger number of clusters may be needed to achieve high accuracy. Additionally, it's worth noting that the accuracy of K-Means clustering is limited by the intrinsic separability of the data, which may not be perfect in all cases.\n",
    "\n",
    "### After scaling the features -  \n",
    "Scaling the data had a significant impact on the performance of K-Means clustering. The overall accuracy increased for all values of K, which suggests that scaling improved the clustering results.\n",
    "\n",
    "In particular, the accuracy of the K-Means clustering (K=9) increased from 55% to 74% after scaling. This is a substantial improvement and indicates that the clusters are better aligned with the true labels.\n",
    "\n",
    "Scaling is an important preprocessing step for many machine learning algorithms, as it can help improve the performance and stability of the models. In this case, scaling helped K-Means clustering to better capture the structure of the data and produce more accurate clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3d5986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7abfe7b",
   "metadata": {},
   "source": [
    "## References -\n",
    "I have referred to the following articles in order to understand the nuances of softmax regression, bagging and boosting techniques.\n",
    "\n",
    "https://towardsdatascience.com/ml-from-scratch-logistic-and-softmax-regression-9f09f49a852c\n",
    "\n",
    "https://www.geeksforgeeks.org/bagging-vs-boosting-in-machine-learning/#\n",
    "\n",
    "https://machinelearningmastery.com/implement-bagging-scratch-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b8a2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
