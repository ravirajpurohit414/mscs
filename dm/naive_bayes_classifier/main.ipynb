{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f02bcc41",
   "metadata": {},
   "source": [
    "#  Rotten Tomatoes Reviews prediction : Naive-Bayes Classifier\n",
    "\n",
    "This dataset is a compilation of movie reviews that were obtained from the well-known movie review website Rotten Tomatoes. The dataset consists of the reviews' text and a corresponding label that specifies whether the review was classified as \"fresh\" or \"rotten\", based on Rotten Tomatoes' proprietary review aggregation system. \n",
    "\n",
    "This dataset is a highly valuable resource for individuals interested in conducting sentiment analysis and natural language processing, including researchers, data analysts, and machine learning practitioners. It contains reviews from a diverse group of critics and publications, encompassing a wide range of movies across various genres and languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "acab4621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ravirajpurohit/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ravirajpurohit/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dc2d24",
   "metadata": {},
   "source": [
    "## -------------------------------------- Load the data --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ca141dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename='./data/rt_reviews.csv'):\n",
    "    \"\"\"\n",
    "    Load the dataset\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename - string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data - numpy array of floats\n",
    "    labels - numpy array of integers\n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    with open(filename, 'r', encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            line = line.split(',')\n",
    "            label, review = line[0], ''.join(line[1:])\n",
    "\n",
    "            labels.append(label)\n",
    "            reviews.append(review)\n",
    "\n",
    "    ## returning from 1st index, 1st line is just column names in the dataset\n",
    "    return np.array(labels[1:]), np.array(reviews[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "032976ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, reviews = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0328b2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((480000,), (480000,))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape, reviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ebbbb7",
   "metadata": {},
   "source": [
    "## -------------------------------------- Split the data --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "dec8d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(labels, reviews):\n",
    "    \"\"\"\n",
    "    DESCRIPTION\n",
    "    \"\"\"\n",
    "    ## assume that labels and reviews are numpy arrays with the same length\n",
    "    data_size = len(labels)\n",
    "\n",
    "    ## shuffle the indices of the data\n",
    "    shuffled_indices = np.random.RandomState(seed=21).permutation(data_size)\n",
    "\n",
    "    ## split the indices into train, validation, and test sets\n",
    "    train_indices = shuffled_indices[:int(0.7 * data_size)]\n",
    "    val_indices = shuffled_indices[int(0.7 * data_size):int(0.8 * data_size)]\n",
    "    test_indices = shuffled_indices[int(0.8 * data_size):]\n",
    "\n",
    "    ## use the indices to extract the corresponding data\n",
    "    train_data = reviews[train_indices]\n",
    "    train_labels = labels[train_indices]\n",
    "\n",
    "    val_data = reviews[val_indices]\n",
    "    val_labels = labels[val_indices]\n",
    "\n",
    "    test_data = reviews[test_indices]\n",
    "    test_labels = labels[test_indices]\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels, val_data, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9e891cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, test_data, test_labels, val_data, val_labels = train_test_val_split(labels, reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a634b17",
   "metadata": {},
   "source": [
    "## -------------------------------------- Data Exploration --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b0d6aaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dist(labels):\n",
    "    \"\"\"\n",
    "    DESCRIPTION\n",
    "    \"\"\"\n",
    "    # assume that train_labels is a numpy array\n",
    "    unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "    for label, count in zip(unique_labels, label_counts):\n",
    "        print(f\"- {label} : {round(100*count/len(labels),2)}%\")\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "338a5c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution in Train dataset\n",
      "- fresh : 50.04%\n",
      "- rotten : 49.96%\n",
      "\n",
      "Class distribution in Test dataset\n",
      "- fresh : 49.75%\n",
      "- rotten : 50.25%\n",
      "\n",
      "Class distribution in Validation dataset\n",
      "- fresh : 50.2%\n",
      "- rotten : 49.8%\n"
     ]
    }
   ],
   "source": [
    "for name, labs in zip(['Train','Test','Validation'],[train_labels, test_labels, val_labels]):\n",
    "    print(f'\\nClass distribution in {name} dataset')\n",
    "    get_data_dist(labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5e06f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "472b0bfa",
   "metadata": {},
   "source": [
    "## -------------------------------------- Data Preprocessing --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "06f35c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    data = np.array([i.replace('\"','').replace('\\n','').strip() for i in data])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "481c1d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- Training data before cleaning ----------------------------- \n",
      "['\" Gloriously daft but with a good deal of heart Fanged Up\\'s Hammer in the slammer shtick has a surprising amount of bite. It\\'s great entertainment for a night in with good friends and a couple of crates of beer -- unless of course you only drink wine.\"\\n'\n",
      " '\" The Back-Up Plan represents a major comeback for Jennifer Lopez. Unfortunately she\\'s come back to making crap. \"\\n'\n",
      " '\" The acting can be so-so the story implausible the camerawork stolid -- none of that really matters if you care about what happens to the characters.\"\\n'\n",
      " ...\n",
      " '\" The film is indeed a bit pat. Sweet and funny - largely thanks to James Corden in the lead role - it\\'s never particularly surprising.\"\\n'\n",
      " ' More concerned with recruiting the testosterone troubled boys of today than it is rewarding fans of yesteryear.\\n'\n",
      " '\" It strives awfully hard for depth but more often than not comes off too shallow.\"\\n']\n"
     ]
    }
   ],
   "source": [
    "print(f'----------------------------- Training data before cleaning ----------------------------- \\n{train_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "930e5941",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = clean_data(train_data)\n",
    "test_data = clean_data(test_data)\n",
    "val_data = clean_data(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c906e0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- Training data after cleaning ----------------------------- \n",
      "[\"Gloriously daft but with a good deal of heart Fanged Up's Hammer in the slammer shtick has a surprising amount of bite. It's great entertainment for a night in with good friends and a couple of crates of beer -- unless of course you only drink wine.\"\n",
      " \"The Back-Up Plan represents a major comeback for Jennifer Lopez. Unfortunately she's come back to making crap.\"\n",
      " 'The acting can be so-so the story implausible the camerawork stolid -- none of that really matters if you care about what happens to the characters.'\n",
      " ...\n",
      " \"The film is indeed a bit pat. Sweet and funny - largely thanks to James Corden in the lead role - it's never particularly surprising.\"\n",
      " 'More concerned with recruiting the testosterone troubled boys of today than it is rewarding fans of yesteryear.'\n",
      " 'It strives awfully hard for depth but more often than not comes off too shallow.']\n"
     ]
    }
   ],
   "source": [
    "print(f'----------------------------- Training data after cleaning ----------------------------- \\n{train_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8f056e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    text = word_tokenize(text)\n",
    "    text = [word for word in text if word.isalpha()]\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    text = ' '.join(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b4b4dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array([preprocess(i) for i in train_data])\n",
    "test_data = np.array([preprocess(i) for i in test_data])\n",
    "# val_data = np.array([preprocess(i) for i in val_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e3b763",
   "metadata": {},
   "source": [
    "## -------------------------------------- Feature Engineering --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaa3ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6f2ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f04d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(reviews):\n",
    "    word_counts = []\n",
    "    for review in reviews:\n",
    "        words = review.split()\n",
    "        word_counts.append(dict(nltk.FreqDist(words)))\n",
    "    feature_matrix = np.zeros((len(reviews), len(vocab)))\n",
    "    for i in range(len(reviews)):\n",
    "        for j, word in enumerate(vocab):\n",
    "            if word in word_counts[i]:\n",
    "                feature_matrix[i, j] = word_counts[i][word]\n",
    "    return feature_matrix\n",
    "\n",
    "all_words = ' '.join(reviews).split()\n",
    "freq_dist = nltk.FreqDist(all_words)\n",
    "vocab = freq_dist.keys()\n",
    "\n",
    "X_train = get_features(train_data)\n",
    "X_test = get_features(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c8dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = get_features(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5253904",
   "metadata": {},
   "source": [
    "## -------------------------------------- Model Training --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dc3bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4deae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train):\n",
    "    n_samples, n_features = X_train.shape\n",
    "    classes = np.unique(y_train)\n",
    "    n_classes = len(classes)\n",
    "    prior_probs = np.zeros(n_classes)\n",
    "    conditional_probs = np.zeros((n_classes, n_features))\n",
    "    for i, label in enumerate(classes):\n",
    "        X_train_label = X_train[y_train == label]\n",
    "        prior_probs[i] = X_train_label.shape[0] / n_samples\n",
    "        conditional_probs[i, :] = (X_train_label.sum(axis=0) + 1) / (X_train_label.sum() + n_features)\n",
    "    return classes, prior_probs, conditional_probs\n",
    "\n",
    "classes, prior_probs, conditional_probs = train(X_train, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dae8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21e242d0",
   "metadata": {},
   "source": [
    "## -------------------------------------- Performance Analysis --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b47844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, classes, prior_probs, conditional_probs):\n",
    "    n_samples, n_features = X_test.shape\n",
    "    y_pred = np.zeros(n_samples)\n",
    "    for i in range(n_samples):\n",
    "        probs = np.zeros(len(classes))\n",
    "        for j, label in enumerate(classes):\n",
    "            probs[j] = np.log(prior_probs[j]) + (np.log(conditional_probs[j, :]) * X_test[i]).sum()\n",
    "        y_pred[i] = classes[np.argmax(probs)]\n",
    "    return y_pred\n",
    "\n",
    "y_pred = predict(X_test, classes, prior_probs, conditional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153e0bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf28ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bdb364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a18ddfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a33b62f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e00d69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
