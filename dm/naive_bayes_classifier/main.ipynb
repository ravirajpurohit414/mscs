{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "128c2d09",
   "metadata": {},
   "source": [
    "#  Rotten Tomatoes Reviews prediction : Naive-Bayes Classifier\n",
    "\n",
    "This dataset is a compilation of movie reviews that were obtained from the well-known movie review website Rotten Tomatoes. The dataset consists of the reviews' text and a corresponding label that specifies whether the review was classified as \"fresh\" or \"rotten\", based on Rotten Tomatoes' proprietary review aggregation system. \n",
    "\n",
    "This dataset is a highly valuable resource for individuals interested in conducting sentiment analysis and natural language processing, including researchers, data analysts, and machine learning practitioners. It contains reviews from a diverse group of critics and publications, encompassing a wide range of movies across various genres and languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e260db61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ravirajpurohit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ravirajpurohit/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/ravirajpurohit/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2bd0ad",
   "metadata": {},
   "source": [
    "## -------------------------------------- Load the data --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e6e90d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename='./data/rt_reviews.csv'):\n",
    "    \"\"\"\n",
    "    Load the dataset\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename - string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    reviews - numpy array of strings : feedback written by viewers\n",
    "    labels - numpy array of strings : fresh / rotten\n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    with open(filename, 'r', encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            line = line.split(',')\n",
    "            label, review = line[0], ''.join(line[1:])\n",
    "\n",
    "            labels.append(label)\n",
    "            reviews.append(review)\n",
    "\n",
    "    ## returning from 1st index; 1st line is just column names in the dataset\n",
    "    return np.array(labels[1:]), np.array(reviews[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4955f9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, reviews = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8339e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((480000,), (480000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape, reviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5009aff0",
   "metadata": {},
   "source": [
    "## -------------------------------------- Split the data --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b2f52b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(labels, reviews):\n",
    "    \"\"\"\n",
    "    splits the dataset into training, testing and validation datasets\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    labels - numpy array of strings : fresh / rotten\n",
    "    reviews - numpy array of strings : feedback written by viewers\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    train_data - numpy array of 70% of reviews chosen randomly\n",
    "    train_labels - numpy array of 70% of labels chosen randomly\n",
    "    test_data - numpy array of 20% of reviews chosen randomly\n",
    "    test_labels - numpy array of 20% of labels chosen randomly\n",
    "    val_data - numpy array of 10% of reviews chosen randomly\n",
    "    val_labels - numpy array of 10% of labels chosen randomly\n",
    "    \"\"\"\n",
    "    ## assume that labels and reviews are numpy arrays with the same length\n",
    "    data_size = len(labels)\n",
    "\n",
    "    ## shuffle the indices of the data\n",
    "    shuffled_indices = np.random.RandomState(seed=21).permutation(data_size)\n",
    "\n",
    "    ## split the indices into train, validation, and test sets\n",
    "    train_indices = shuffled_indices[:int(0.7 * data_size)]\n",
    "    val_indices = shuffled_indices[int(0.7 * data_size):int(0.8 * data_size)]\n",
    "    test_indices = shuffled_indices[int(0.8 * data_size):]\n",
    "\n",
    "    ## use the indices to extract the corresponding data\n",
    "    train_data = reviews[train_indices]\n",
    "    train_labels = labels[train_indices]\n",
    "\n",
    "    val_data = reviews[val_indices]\n",
    "    val_labels = labels[val_indices]\n",
    "\n",
    "    test_data = reviews[test_indices]\n",
    "    test_labels = labels[test_indices]\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels, val_data, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a9a20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, test_data, test_labels, val_data, val_labels = train_test_val_split(labels, reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acc1a69",
   "metadata": {},
   "source": [
    "## -------------------------------------- Data Exploration --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22b4b146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dist(labels):\n",
    "    \"\"\"\n",
    "    prints the distribution of labels in a dataset\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    labels - numpy array of labels\n",
    "    \"\"\"\n",
    "    # assume that train_labels is a numpy array\n",
    "    unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "    for label, count in zip(unique_labels, label_counts):\n",
    "        print(f\"- {label} : {round(100*count/len(labels),2)}%\")\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3e57c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution in Train dataset\n",
      "- fresh : 50.04%\n",
      "- rotten : 49.96%\n",
      "\n",
      "Class distribution in Test dataset\n",
      "- fresh : 49.75%\n",
      "- rotten : 50.25%\n",
      "\n",
      "Class distribution in Validation dataset\n",
      "- fresh : 50.2%\n",
      "- rotten : 49.8%\n"
     ]
    }
   ],
   "source": [
    "for name, labs in zip(['Train','Test','Validation'],[train_labels, test_labels, val_labels]):\n",
    "    print(f'\\nClass distribution in {name} dataset')\n",
    "    get_data_dist(labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b057f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64295736",
   "metadata": {},
   "source": [
    "## -------------------------------------- Data Preprocessing --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "885a154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    \"\"\"\n",
    "    clean data from new line character, empty spaces on the ends, and inverted commas\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data - numpy array of strings\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data - numpy array of strings\n",
    "    \"\"\"\n",
    "    data = np.array([i.replace('\"','').replace('\\n','').strip() for i in data])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78685a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- Training data before cleaning ----------------------------- \n",
      "['\" Gloriously daft but with a good deal of heart Fanged Up\\'s Hammer in the slammer shtick has a surprising amount of bite. It\\'s great entertainment for a night in with good friends and a couple of crates of beer -- unless of course you only drink wine.\"\\n'\n",
      " '\" The Back-Up Plan represents a major comeback for Jennifer Lopez. Unfortunately she\\'s come back to making crap. \"\\n'\n",
      " '\" The acting can be so-so the story implausible the camerawork stolid -- none of that really matters if you care about what happens to the characters.\"\\n'\n",
      " ...\n",
      " '\" The film is indeed a bit pat. Sweet and funny - largely thanks to James Corden in the lead role - it\\'s never particularly surprising.\"\\n'\n",
      " ' More concerned with recruiting the testosterone troubled boys of today than it is rewarding fans of yesteryear.\\n'\n",
      " '\" It strives awfully hard for depth but more often than not comes off too shallow.\"\\n']\n"
     ]
    }
   ],
   "source": [
    "print(f'----------------------------- Training data before cleaning ----------------------------- \\n{train_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4709d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = clean_data(train_data)\n",
    "test_data = clean_data(test_data)\n",
    "val_data = clean_data(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "295502d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- Training data after cleaning ----------------------------- \n",
      "[\"Gloriously daft but with a good deal of heart Fanged Up's Hammer in the slammer shtick has a surprising amount of bite. It's great entertainment for a night in with good friends and a couple of crates of beer -- unless of course you only drink wine.\"\n",
      " \"The Back-Up Plan represents a major comeback for Jennifer Lopez. Unfortunately she's come back to making crap.\"\n",
      " 'The acting can be so-so the story implausible the camerawork stolid -- none of that really matters if you care about what happens to the characters.'\n",
      " ...\n",
      " \"The film is indeed a bit pat. Sweet and funny - largely thanks to James Corden in the lead role - it's never particularly surprising.\"\n",
      " 'More concerned with recruiting the testosterone troubled boys of today than it is rewarding fans of yesteryear.'\n",
      " 'It strives awfully hard for depth but more often than not comes off too shallow.']\n"
     ]
    }
   ],
   "source": [
    "print(f'----------------------------- Training data after cleaning ----------------------------- \\n{train_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e58119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \n",
    "              \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', \n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', \n",
    "              'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "              'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', \n",
    "              'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', \n",
    "              'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', \n",
    "              'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', \n",
    "              'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', \n",
    "              'about', 'against', 'between', 'into', 'through', 'during', 'before', \n",
    "              'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', \n",
    "              'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', \n",
    "              'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', \n",
    "              'each', 'few', 'more', 'most', 'other', 'some', 'such', 'nor', \n",
    "              'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', \n",
    "              'can', 'will', 'just', 'don', 'now', \n",
    "              'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", \n",
    "              'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \n",
    "              \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', \n",
    "              'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \n",
    "              'wasn', \"wasn't\", 'weren', \"weren't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5b72713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    steps like -\n",
    "    converts all characters to lower\n",
    "    splits sentence to words\n",
    "    removes non-alphabetical words\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text - string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    text - string (preprocessed)\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    ## word_tokenize splits a sentence into words linguistically\n",
    "    text = word_tokenize(text)\n",
    "    text = [word for word in text if (word.isalpha() and word not in stop_words)]\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8934316",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess all the datasets\n",
    "train_data = np.array([preprocess(i) for i in train_data])\n",
    "test_data = np.array([preprocess(i) for i in test_data])\n",
    "val_data = np.array([preprocess(i) for i in val_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7464114a",
   "metadata": {},
   "source": [
    "## -------------------------------------- Create Vocabulary --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4343be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lemmatizer instance to keep words with same meaning as same signature\n",
    "## for example - converts 'went' to 'go'\n",
    "lemm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3fabfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize dictionaries for vocabulary\n",
    "vocab_all = defaultdict(int)\n",
    "vocab_fresh = defaultdict(int)\n",
    "vocab_rotten = defaultdict(int)\n",
    "\n",
    "for ind, item in enumerate(train_data):\n",
    "    words_unq = []\n",
    "    \n",
    "    ## split sentence by words linguistically\n",
    "    words = nltk.word_tokenize(item)\n",
    "\n",
    "    for word in words:\n",
    "        ## lemmatize words to create a good vocabulary\n",
    "        words_unq.append(lemm.lemmatize(word, pos='v'))\n",
    "        words_unq.append(word)\n",
    "    \n",
    "    ## only keep unique set of words, remove duplicates\n",
    "    words_unq = set(words_unq)\n",
    "        \n",
    "    if train_labels[ind] == 'fresh':\n",
    "        for word in words:\n",
    "            vocab_all[word] += 1\n",
    "            vocab_fresh[word] += 1\n",
    "    else:\n",
    "        for word in words:\n",
    "            vocab_all[word] += 1\n",
    "            vocab_rotten[word] += 1\n",
    "            \n",
    "vocab_all = {key:value for key, value in vocab_all.items() if value > 10}\n",
    "vocab_fresh = {key:value for key, value in vocab_fresh.items() if value > 5}\n",
    "vocab_fresh = {key:value for key, value in vocab_fresh.items() if value > 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65b5f0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22941, 57532, 54929)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_all), len(vocab_fresh), len(vocab_rotten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77109e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25a4d4b6",
   "metadata": {},
   "source": [
    "## -------------------------------------- Model Training-Testing --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d71314a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['fresh', 'rotten'], dtype='<U6'), array([168145, 167855]))\n"
     ]
    }
   ],
   "source": [
    "## find the counts of fresh and rotten labels in the dataset\n",
    "counts_fresh, counts_rotten = np.unique(train_labels, return_counts=True)[1]\n",
    "print(np.unique(train_labels, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44ba7b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data):\n",
    "    \"\"\"\n",
    "    predicts the label for a review using naive bayes theroem\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data - string : movie review\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    label - string : fresh/rotten\n",
    "    \"\"\"\n",
    "    ## define the local constants\n",
    "    prob_fresh, prob_rotten, label = 1, 1, None\n",
    "    \n",
    "    base1 = (len(vocab_all) + counts_fresh)\n",
    "    base2 = (len(vocab_all) + counts_rotten)\n",
    "    \n",
    "    for word in data.split():\n",
    "        if word in vocab_fresh:\n",
    "            prob_fresh *= (vocab_fresh[word] / base1)\n",
    "        else:\n",
    "            prob_fresh /= base1\n",
    "\n",
    "        if word in vocab_rotten:\n",
    "            prob_rotten *= (vocab_rotten[word] / base2)\n",
    "        else:\n",
    "            prob_rotten /= base2\n",
    "    \n",
    "    if prob_fresh > prob_rotten:\n",
    "        label = 'fresh'\n",
    "    else:\n",
    "        label = 'rotten'\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c01ff65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_smoothing(data):\n",
    "    \"\"\"\n",
    "    predicts the label for a review using naive bayes theroem\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data - string : movie review\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    label - string : fresh/rotten\n",
    "    \"\"\"\n",
    "    ## define the local constants\n",
    "    prob_fresh, prob_rotten, label = 1, 1, None\n",
    "    \n",
    "    base1 = (len(vocab_all) + counts_fresh)\n",
    "    base2 = (len(vocab_all) + counts_rotten)\n",
    "    \n",
    "    for word in data.split():\n",
    "        if word in vocab_fresh:\n",
    "            prob_fresh *= ((vocab_fresh[word] + 1) / (base1 + 1))\n",
    "        else:\n",
    "            prob_fresh /= (base1 + 1)\n",
    "\n",
    "        if word in vocab_rotten:\n",
    "            prob_rotten *= ((vocab_rotten[word] + 1) / (base2 + 1))\n",
    "        else:\n",
    "            prob_rotten /= (base2 + 1)\n",
    "    \n",
    "    if prob_fresh > prob_rotten:\n",
    "        label = 'fresh'\n",
    "    else:\n",
    "        label = 'rotten'\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cde2de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = []\n",
    "pred_test = []\n",
    "pred_val = []\n",
    "\n",
    "for data in train_data:\n",
    "    pred_train.append(predict(data))\n",
    "\n",
    "for data in test_data:\n",
    "    pred_test.append(predict(data))\n",
    "    \n",
    "for data in val_data:\n",
    "    pred_val.append(predict(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7745ca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_smooth = []\n",
    "pred_test_smooth = []\n",
    "pred_val_smooth = []\n",
    "\n",
    "for data in train_data:\n",
    "    pred_train_smooth.append(predict(data))\n",
    "\n",
    "for data in test_data:\n",
    "    pred_test_smooth.append(predict(data))\n",
    "    \n",
    "for data in val_data:\n",
    "    pred_val_smooth.append(predict(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b69767c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51d95178",
   "metadata": {},
   "source": [
    "## -------------------------------------- Performance Analysis --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e1b9f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(pred, actual):\n",
    "    \"\"\"\n",
    "    return prediction accuracy\n",
    "    \n",
    "    pred - list of predicted labels\n",
    "    actual - list of actual labels\n",
    "    \"\"\"\n",
    "    \n",
    "    accuracy = round(100*np.mean(pred == actual),2)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "850288d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Training Dataset - 81.54%\n",
      "Accuracy for Testing Dataset - 79.25%\n",
      "Accuracy for Validation Dataset - 79.21%\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy for Training Dataset - {calc_accuracy(pred_train, train_labels)}%')\n",
    "print(f'Accuracy for Testing Dataset - {calc_accuracy(pred_test, test_labels)}%')\n",
    "print(f'Accuracy for Validation Dataset - {calc_accuracy(pred_val, val_labels)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "be873707",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy for Training Dataset - {calc_accuracy(pred_train_smooth, train_labels)}%')\n",
    "print(f'Accuracy for Testing Dataset - {calc_accuracy(pred_test_smooth, test_labels)}%')\n",
    "print(f'Accuracy for Validation Dataset - {calc_accuracy(pred_val_smooth, val_labels)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eceb98",
   "metadata": {},
   "source": [
    "Accuracy for Training Dataset - 82.59%\n",
    "\n",
    "Accuracy for Testing Dataset - 79.8%\n",
    "\n",
    "Accuracy for Validation Dataset - 79.93%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc675f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e85f85c",
   "metadata": {},
   "source": [
    "## -------------------------------------- Probability of THE occurance ---------------------------------\n",
    "\n",
    "P[“the”] = num of documents containing ‘the’ / num of all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2b733097",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, reviews = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1b42356b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(100*sum([True for i in reviews if 'the' in i.lower().split()]) / len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "52b30acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe probability for the word \"the\" as asked in the assignment is 63%\\n'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The probability for the word \"the\" as asked in the assignment is 63%\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41145707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "879da5aa",
   "metadata": {},
   "source": [
    "## -------------------------------------- Conditional Probability --------------------------------------\n",
    "Conditional Probability based on the sentiment\n",
    "\n",
    "Calculate P[“the”|Positive] = Number of positive documents containing “the” / num of all positive review documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "003d9d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_doc = reviews[labels=='fresh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "04f5bb7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.56"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(100*sum([True for i in pos_doc if 'the' in i.lower().split()]) / len(pos_doc),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d27d41aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe conditional probability for the word \"the\" as asked in the assignment is 63.56%\\n'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The conditional probability for the word \"the\" as asked in the assignment is 63.56%\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f710aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1a6883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
